{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement_Learning_03.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM8tzDi5VBzcxoq20W6waaH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rpdieego/Reinforcement_Learning/blob/master/Reinforcement_Learning_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-efU43F10EV",
        "colab_type": "text"
      },
      "source": [
        "# Monte Carlo Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dkr33qv_Okex",
        "colab_type": "text"
      },
      "source": [
        "#### Monte Carlo Policy Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXs2_sQK12KY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import relevant packages\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycWH5ukePL6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# definitions\n",
        "\n",
        "#convergence parameter\n",
        "conv_parameter = 10e-4\n",
        "\n",
        "# discount factor\n",
        "gamma = 0.9\n",
        "\n",
        "# Action Space\n",
        "ALL_POSSIBLE_ACTIONS = ('U','D','L','R')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaMlEeNQO2s7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#grid world class\n",
        "\n",
        "class Grid: # Environment\n",
        "  def __init__ (self, rows, cols, start):\n",
        "    self.rows = rows\n",
        "    self.cols = cols\n",
        "    self.i = start[0]\n",
        "    self.j = start[1]\n",
        "\n",
        "  def set(self, rewards, actions):\n",
        "    # rewards is a dict of: (i,j): r\n",
        "    # actions is a dict of: (i,j): A\n",
        "    self.rewards = rewards\n",
        "    self.actions = actions\n",
        "\n",
        "  def set_state(self,s):\n",
        "    #force state\n",
        "    self.i = s[0]\n",
        "    self.j = s[1]\n",
        "\n",
        "  def current_state(self):\n",
        "    #return the current state\n",
        "    return (self.i, self.j)\n",
        "  \n",
        "  def is_terminal(self, s):\n",
        "    #if state is not listed in the actions dictionary, it means it's a terminal state\n",
        "    return s not in self.actions\n",
        "\n",
        "  def get_next_state(self, s, a):\n",
        "    i, j = s[0], s[1]\n",
        "    if a in self.actions[(i,j)]:\n",
        "      if a == 'U':\n",
        "        i -= 1\n",
        "      elif a =='D':\n",
        "        i += 1\n",
        "      elif a == 'R':\n",
        "        j += 1\n",
        "      elif a == 'L':\n",
        "        j -= 1\n",
        "    return i,j\n",
        "\n",
        "  def move(self, action):\n",
        "    # check if legal move first\n",
        "    if action in self.actions[(self.i, self.j)]:\n",
        "      if action == 'U':\n",
        "        self.i -= 1\n",
        "      elif action == 'D':\n",
        "        self.i += 1\n",
        "      elif action == 'R':\n",
        "        self.j += 1\n",
        "      elif action == 'L':\n",
        "        self.j -= 1\n",
        "    return self.rewards.get((self.i, self.j), 0)\n",
        "\n",
        "  def undo_move(self, action):\n",
        "    if action == 'U':\n",
        "      self.i += 1\n",
        "    elif action == 'D':\n",
        "      self.i -= 1\n",
        "    elif action == 'R':\n",
        "      self.j -= 1\n",
        "    elif action == 'L':\n",
        "      self.j += 1\n",
        "    #should never happen\n",
        "    assert(self.current_state() in self.all_states())\n",
        "  \n",
        "  def game_over(self):\n",
        "    #true if in a state where no actions are possible\n",
        "    return (self.i,self.j) not in self.actions\n",
        "\n",
        "  def all_states(self):\n",
        "    #either a position that has possible next actions or a positions that yields a reward\n",
        "    return set(self.actions.keys()) | set(self.rewards.keys())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAosBpBQPAjN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define grid environment\n",
        "\n",
        "def standard_grid():\n",
        "  g = Grid(3, 4, (2,0))\n",
        "  rewards = {(0,3): 1, (1,3): -1}\n",
        "  actions = {\n",
        "      (0,0): ('D','R'),\n",
        "      (0,1): ('L','R'),\n",
        "      (0,2): ('L','D','R'),\n",
        "      (1,0): ('U','D'),\n",
        "      (1,2): ('U', 'D', 'R'),\n",
        "      (2,0): ('U','R'),\n",
        "      (2,1): ('L','R'),\n",
        "      (2,2): ('L','R','U'),\n",
        "      (2,3): ('L','U')\n",
        "  }\n",
        "  g.set(rewards, actions)\n",
        "  return g"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-T2rE2rjQ-qP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define grid environment with step penalties\n",
        "\n",
        "def negative_grid(step_cost=-0.1):\n",
        "  # in this game we want to try to minimize the number of moves\n",
        "  # so we will penalize every move\n",
        "  g = standard_grid()\n",
        "  g.rewards.update({\n",
        "    (0, 0): step_cost,\n",
        "    (0, 1): step_cost,\n",
        "    (0, 2): step_cost,\n",
        "    (1, 0): step_cost,\n",
        "    (1, 2): step_cost,\n",
        "    (2, 0): step_cost,\n",
        "    (2, 1): step_cost,\n",
        "    (2, 2): step_cost,\n",
        "    (2, 3): step_cost,\n",
        "  })\n",
        "  return g"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLPSs2j0PBC7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# auxiliar function to print values\n",
        "def print_values(V,g):\n",
        "  for i in range(g.rows):\n",
        "    print('-------------------------------')\n",
        "    for j in range(g.cols):\n",
        "      v = V.get((i,j),0)\n",
        "      if v>= 0:\n",
        "        print(\" %.2f|\" % v, end=\"\")\n",
        "      else:\n",
        "        print(\"%.2f|\" % v, end=\"\") # - sign take up an extra space\n",
        "    print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suzZ_orOQQoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# auxiliar function to print policy\n",
        "def print_policy(P,g):\n",
        "  print('Policy \\n')\n",
        "  for i in range(g.rows):\n",
        "    print('-----------------------------')\n",
        "    for j in range(g.cols):\n",
        "      a = P.get((i,j),' ')\n",
        "      print(\"  %s  |\" % a, end='')\n",
        "    print('')\n",
        "  print('\\n')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6p_tgPZQTzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_game(grid, policy):\n",
        "\n",
        "  #returns a list of states and corresponding returns\n",
        "\n",
        "  #reset game to restart at random position\n",
        "  #we need to do this, because given our current deterministic policy\n",
        "  #we would never end up at certain states, but we still want to measure their value\n",
        "\n",
        "  start_states = list(grid.actions.keys())\n",
        "  start_idx = np.random.choice(len(start_states))\n",
        "  grid.set_state(start_states[start_idx])\n",
        "\n",
        "  s = grid.current_state()\n",
        "  # list of tuples of (state, reward)\n",
        "  states_and_rewards = [(s,0)]\n",
        "  while not  grid.game_over():\n",
        "    a = policy[s]\n",
        "    r = grid.move(a)\n",
        "    s = grid.current_state()\n",
        "    states_and_rewards.append((s,r))\n",
        "\n",
        "  #Calculate teh returns by working backwards from the terminal state\n",
        "  G = 0\n",
        "  states_and_returns = []\n",
        "  first = True\n",
        "  for s,r in reversed(states_and_rewards):\n",
        "    # value of terminal state is 0 by definition\n",
        "    #we should ignores the first state we encounter (terminal)\n",
        "    # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
        "    if first:\n",
        "      first = False\n",
        "    else:\n",
        "      states_and_returns.append((s,G))\n",
        "    \n",
        "    G = r + gamma*G\n",
        "\n",
        "  # we want it to be in order of state visited\n",
        "  states_and_returns.reverse()\n",
        "\n",
        "  return states_and_returns\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUMgMF7WSW-9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "1a1f854c-fe76-4e89-8019-22bc695ce1ea"
      },
      "source": [
        "### main\n",
        "\n",
        "# use the standard grid again (0 for every step) so that we can compare \n",
        "# to the iterative policy evaluation\n",
        "\n",
        "grid = standard_grid()\n",
        "\n",
        "# print rewards\n",
        "print('Rewards:')\n",
        "print_values(grid.rewards, grid)\n",
        "print('\\n')\n",
        "\n",
        "# state -> action\n",
        "policy = {\n",
        "    (2,0): 'U',\n",
        "    (1,0): 'U',\n",
        "    (0,0): 'R',\n",
        "    (0,1): 'R',\n",
        "    (0,2): 'R',\n",
        "    (1,2): 'R',\n",
        "    (2,1): 'R',\n",
        "    (2,2): 'R',\n",
        "    (2,3): 'U',\n",
        "}\n",
        "\n",
        "# initialize V(s) and returns\n",
        "V = {}\n",
        "#dictionary of state => list of returns we've recieved\n",
        "returns = {}\n",
        "states = grid.all_states()\n",
        "for s in states:\n",
        "  if s in grid.actions:\n",
        "    returns[s] = []\n",
        "  else:\n",
        "    #terminal state or state we can't otherwise get to\n",
        "    V[s] = 0\n",
        "\n",
        "# repeat\n",
        "for t in range(100):\n",
        "\n",
        "  #generate an episode using pi\n",
        "  states_and_returns = play_game(grid, policy)\n",
        "  seen_states = set()\n",
        "  for s, G in states_and_returns:\n",
        "    # ' first visit ' MC policy evaluation\n",
        "    # check if we already have seen s\n",
        "    if s not in seen_states:\n",
        "      returns[s].append(G)\n",
        "      V[s] = np.mean(returns[s])\n",
        "      seen_states.add(s)\n",
        "\n",
        "print('Values:')\n",
        "print_values(V,grid)\n",
        "print('\\n')\n",
        "print('Policy:')\n",
        "print_policy(policy,grid)\n",
        "print('\\n')\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rewards:\n",
            "-------------------------------\n",
            " 0.00| 0.00| 0.00| 1.00|\n",
            "-------------------------------\n",
            " 0.00| 0.00| 0.00|-1.00|\n",
            "-------------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "\n",
            "\n",
            "Values:\n",
            "-------------------------------\n",
            " 0.81| 0.90| 1.00| 0.00|\n",
            "-------------------------------\n",
            " 0.73| 0.00|-1.00| 0.00|\n",
            "-------------------------------\n",
            " 0.66|-0.81|-0.90|-1.00|\n",
            "\n",
            "\n",
            "Policy:\n",
            "Policy \n",
            "\n",
            "-----------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "-----------------------------\n",
            "  U  |     |  R  |     |\n",
            "-----------------------------\n",
            "  U  |  R  |  R  |  U  |\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRE4Xkq3f3-Q",
        "colab_type": "text"
      },
      "source": [
        "#### Monte Carlo Policy Evaluation in the Grid World\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEEAiw28f7-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_action(a):\n",
        "  #choose given a with probability 0.5\n",
        "  #choose some other a' != with probability 0.5/3\n",
        "  p = np.random.random()\n",
        "  if p < 0.5:\n",
        "    return a\n",
        "  else:\n",
        "    tmp = list(ALL_POSSIBLE_ACTIONS)\n",
        "    tmp.remove(a)\n",
        "    return np.random.choice(tmp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYmmYXlyh2KT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_game(grid, policy):\n",
        "\n",
        "  #returns a list of states and corresponding returns\n",
        "\n",
        "  #reset game to restart at random position\n",
        "  #we need to do this, because given our current deterministic policy\n",
        "  #we would never end up at certain states, but we still want to measure their value\n",
        "\n",
        "  start_states = list(grid.actions.keys())\n",
        "  start_idx = np.random.choice(len(start_states))\n",
        "  grid.set_state(start_states[start_idx])\n",
        "\n",
        "  s = grid.current_state()\n",
        "  # list of tuples of (state, reward)\n",
        "  states_and_rewards = [(s,0)]\n",
        "  while not  grid.game_over():\n",
        "    a = policy[s]\n",
        "    # possibly choose another action\n",
        "    a = random_action(a)\n",
        "    r = grid.move(a)\n",
        "    s = grid.current_state()\n",
        "    states_and_rewards.append((s,r))\n",
        "\n",
        "  #Calculate teh returns by working backwards from the terminal state\n",
        "  G = 0\n",
        "  states_and_returns = []\n",
        "  first = True\n",
        "  for s,r in reversed(states_and_rewards):\n",
        "    # value of terminal state is 0 by definition\n",
        "    #we should ignores the first state we encounter (terminal)\n",
        "    # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
        "    if first:\n",
        "      first = False\n",
        "    else:\n",
        "      states_and_returns.append((s,G))\n",
        "    \n",
        "    G = r + gamma*G\n",
        "\n",
        "  # we want it to be in order of state visited\n",
        "  states_and_returns.reverse()\n",
        "\n",
        "  return states_and_returns\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YizA2baYiTXp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "72acb8cd-e352-4c5d-9efd-eaa79d4fb69d"
      },
      "source": [
        "### main\n",
        "\n",
        "# use the standard grid again (0 for every step) so that we can compare \n",
        "# to the iterative policy evaluation\n",
        "\n",
        "grid = standard_grid()\n",
        "\n",
        "# print rewards\n",
        "print('Rewards:')\n",
        "print_values(grid.rewards, grid)\n",
        "print('\\n')\n",
        "\n",
        "# state -> action\n",
        "policy = {\n",
        "    (2,0): 'U',\n",
        "    (1,0): 'U',\n",
        "    (0,0): 'R',\n",
        "    (0,1): 'R',\n",
        "    (0,2): 'R',\n",
        "    (1,2): 'U',\n",
        "    (2,1): 'L',\n",
        "    (2,2): 'U',\n",
        "    (2,3): 'L',\n",
        "}\n",
        "\n",
        "# initialize V(s) and returns\n",
        "V = {}\n",
        "#dictionary of state => list of returns we've recieved\n",
        "returns = {}\n",
        "states = grid.all_states()\n",
        "for s in states:\n",
        "  if s in grid.actions:\n",
        "    returns[s] = []\n",
        "  else:\n",
        "    #terminal state or state we can't otherwise get to\n",
        "    V[s] = 0\n",
        "\n",
        "# repeat\n",
        "for t in range(100):\n",
        "\n",
        "  #generate an episode using pi\n",
        "  states_and_returns = play_game(grid, policy)\n",
        "  seen_states = set()\n",
        "  for s, G in states_and_returns:\n",
        "    # ' first visit ' MC policy evaluation\n",
        "    # check if we already have seen s\n",
        "    if s not in seen_states:\n",
        "      returns[s].append(G)\n",
        "      V[s] = np.mean(returns[s])\n",
        "      seen_states.add(s)\n",
        "\n",
        "print('Values:')\n",
        "print_values(V,grid)\n",
        "print('\\n')\n",
        "print('Policy:')\n",
        "print_policy(policy,grid)\n",
        "print('\\n')\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rewards:\n",
            "-------------------------------\n",
            " 0.00| 0.00| 0.00| 1.00|\n",
            "-------------------------------\n",
            " 0.00| 0.00| 0.00|-1.00|\n",
            "-------------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "\n",
            "\n",
            "Values:\n",
            "-------------------------------\n",
            " 0.48| 0.59| 0.71| 0.00|\n",
            "-------------------------------\n",
            " 0.37| 0.00| 0.22| 0.00|\n",
            "-------------------------------\n",
            " 0.31| 0.13| 0.08|-0.25|\n",
            "\n",
            "\n",
            "Policy:\n",
            "Policy \n",
            "\n",
            "-----------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "-----------------------------\n",
            "  U  |     |  U  |     |\n",
            "-----------------------------\n",
            "  U  |  L  |  U  |  L  |\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NKboL96w5xE",
        "colab_type": "text"
      },
      "source": [
        "#### Monte Carlo Control Problem - Exploring Starts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDDB3LfIiqRt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_game(grid, policy):\n",
        "  # returns a list of states and corresponding returns\n",
        "\n",
        "  # reset game to start at a random position\n",
        "  # we need to do this if we have a deterministic policy\n",
        "  # we would never end up at certain states, but we still want to measure their value\n",
        "  # this is called the \"exploring starts\" method\n",
        "  start_states = list(grid.actions.keys())\n",
        "  start_idx = np.random.choice(len(start_states))\n",
        "  grid.set_state(start_states[start_idx])\n",
        "\n",
        "  s = grid.current_state()\n",
        "  a = np.random.choice(ALL_POSSIBLE_ACTIONS) # first action is uniformly random\n",
        "\n",
        "  # be aware of the timing\n",
        "  # each triple is s(t), a(t), r(t)\n",
        "  # but r(t) results from taking action a(t-1) from s(t-1) and landing in s(t)\n",
        "  states_actions_rewards = [(s, a, 0)]\n",
        "  seen_states = set()\n",
        "  seen_states.add(grid.current_state())\n",
        "  num_steps = 0\n",
        "  while True:\n",
        "    r = grid.move(a)\n",
        "    num_steps += 1\n",
        "    s = grid.current_state()\n",
        "\n",
        "    if s in seen_states:\n",
        "      # hack so that we don't end up in an infinitely long episode\n",
        "      # bumping into the wall repeatedly\n",
        "      # if num_steps == 1 -> bumped into a wall and haven't moved anywhere\n",
        "      #   reward = -10\n",
        "      # else:\n",
        "      #   reward = falls off by 1 / num_steps\n",
        "      reward = -10. / num_steps\n",
        "      states_actions_rewards.append((s, None, reward))\n",
        "      break\n",
        "    elif grid.game_over():\n",
        "      states_actions_rewards.append((s, None, r))\n",
        "      break\n",
        "    else:\n",
        "      a = policy[s]\n",
        "      states_actions_rewards.append((s, a, r))\n",
        "    seen_states.add(s)\n",
        "\n",
        "  # calculate the returns by working backwards from the terminal state\n",
        "  G = 0\n",
        "  states_actions_returns = []\n",
        "  first = True\n",
        "  for s, a, r in reversed(states_actions_rewards):\n",
        "    # the value of the terminal state is 0 by definition\n",
        "    # we should ignore the first state we encounter\n",
        "    # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
        "    if first:\n",
        "      first = False\n",
        "    else:\n",
        "      states_actions_returns.append((s, a, G))\n",
        "    G = r + gamma*G\n",
        "  states_actions_returns.reverse() # we want it to be in order of state visited\n",
        "  return states_actions_returns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2mXN7DV2iSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_dict(d):\n",
        "  # returns the argmax (key) and max (value) from a dictionary\n",
        "  # put this into a function since we are using it so often\n",
        "  max_key = None\n",
        "  max_val = float('-inf')\n",
        "  for k, v in d.items():\n",
        "    if v > max_val:\n",
        "      max_val = v\n",
        "      max_key = k\n",
        "  return max_key, max_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNbz9hoj7YUZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "outputId": "a09a92da-bb65-4c7f-acae-6ecc04a689fa"
      },
      "source": [
        "# use the standard grid again (0 for every step) so that we can compare\n",
        "# to iterative policy evaluation\n",
        "# grid = standard_grid()\n",
        "# try the negative grid too, to see if agent will learn to go past the \"bad spot\"\n",
        "# in order to minimize number of steps\n",
        "grid = negative_grid(step_cost=-0.5)\n",
        "\n",
        "# print rewards\n",
        "print(\"rewards:\")\n",
        "print_values(grid.rewards, grid)\n",
        "\n",
        "# state -> action\n",
        "# initialize a random policy\n",
        "policy = {}\n",
        "for s in grid.actions.keys():\n",
        "  policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
        "\n",
        "# initialize Q(s,a) and returns\n",
        "Q = {}\n",
        "returns = {} # dictionary of state -> list of returns we've received\n",
        "states = grid.all_states()\n",
        "for s in states:\n",
        "  if s in grid.actions: # not a terminal state\n",
        "    Q[s] = {}\n",
        "    for a in ALL_POSSIBLE_ACTIONS:\n",
        "      Q[s][a] = 0 # needs to be initialized to something so we can argmax it\n",
        "      returns[(s,a)] = []\n",
        "  else:\n",
        "    # terminal state or state we can't otherwise get to\n",
        "    pass\n",
        "\n",
        "# repeat until convergence\n",
        "deltas = []\n",
        "for t in range(5000):\n",
        "\n",
        "  # generate an episode using pi\n",
        "  biggest_change = 0\n",
        "  states_actions_returns = play_game(grid, policy)\n",
        "  seen_state_action_pairs = set()\n",
        "  for s, a, G in states_actions_returns:\n",
        "    # check if we have already seen s\n",
        "    # called \"first-visit\" MC policy evaluation\n",
        "    sa = (s, a)\n",
        "    if sa not in seen_state_action_pairs:\n",
        "      old_q = Q[s][a]\n",
        "      returns[sa].append(G)\n",
        "      Q[s][a] = np.mean(returns[sa])\n",
        "      biggest_change = max(biggest_change, np.abs(old_q - Q[s][a]))\n",
        "      seen_state_action_pairs.add(sa)\n",
        "  deltas.append(biggest_change)\n",
        "\n",
        "  # update policy\n",
        "  for s in policy.keys():\n",
        "    policy[s] = max_dict(Q[s])[0]\n",
        "\n",
        "plt.plot(deltas)\n",
        "plt.show()\n",
        "\n",
        "print(\"final policy:\")\n",
        "print_policy(policy, grid)\n",
        "\n",
        "# find V\n",
        "V = {}\n",
        "for s, Qs in Q.items():\n",
        "  V[s] = max_dict(Q[s])[1]\n",
        "\n",
        "print(\"final values:\")\n",
        "print_values(V, grid)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rewards:\n",
            "-------------------------------\n",
            "-0.50|-0.50|-0.50| 1.00|\n",
            "-------------------------------\n",
            "-0.50| 0.00|-0.50|-1.00|\n",
            "-------------------------------\n",
            "-0.50|-0.50|-0.50|-0.50|\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "final policy:\n",
            "Policy \n",
            "\n",
            "-----------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "-----------------------------\n",
            "  U  |     |  U  |     |\n",
            "-----------------------------\n",
            "  U  |  R  |  U  |  U  |\n",
            "\n",
            "\n",
            "final values:\n",
            "-------------------------------\n",
            "-1.23|-0.71| 1.00| 0.00|\n",
            "-------------------------------\n",
            "-1.86| 0.00|-0.49| 0.00|\n",
            "-------------------------------\n",
            "-2.16|-1.93|-1.10|-1.00|\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cM2GxDVNAZP",
        "colab_type": "text"
      },
      "source": [
        "#### Monte Carlo Control Problem Without Exploring Starts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnaUQhA08_69",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_action(a, eps=0.1):\n",
        "  # choose given a with probability 1 - eps + eps/4\n",
        "  # choose some other a' != a with probability eps/4\n",
        "  p = np.random.random()\n",
        "  # if p < (1 - eps + eps/len(ALL_POSSIBLE_ACTIONS)):\n",
        "  #   return a\n",
        "  # else:\n",
        "  #   tmp = list(ALL_POSSIBLE_ACTIONS)\n",
        "  #   tmp.remove(a)\n",
        "  #   return np.random.choice(tmp)\n",
        "  #\n",
        "  # this is equivalent to the above\n",
        "  if p < (1 - eps):\n",
        "    return a\n",
        "  else:\n",
        "    return np.random.choice(ALL_POSSIBLE_ACTIONS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXLVGasUNy9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_game(grid, policy):\n",
        "  # returns a list of states and corresponding returns\n",
        "  # in this version we will NOT use \"exploring starts\" method\n",
        "  # instead we will explore using an epsilon-soft policy\n",
        "  s = (2, 0)\n",
        "  grid.set_state(s)\n",
        "  a = random_action(policy[s])\n",
        "\n",
        "  # be aware of the timing\n",
        "  # each triple is s(t), a(t), r(t)\n",
        "  # but r(t) results from taking action a(t-1) from s(t-1) and landing in s(t)\n",
        "  states_actions_rewards = [(s, a, 0)]\n",
        "  while True:\n",
        "    r = grid.move(a)\n",
        "    s = grid.current_state()\n",
        "    if grid.game_over():\n",
        "      states_actions_rewards.append((s, None, r))\n",
        "      break\n",
        "    else:\n",
        "      a = random_action(policy[s]) # the next state is stochastic\n",
        "      states_actions_rewards.append((s, a, r))\n",
        "\n",
        "  # calculate the returns by working backwards from the terminal state\n",
        "  G = 0\n",
        "  states_actions_returns = []\n",
        "  first = True\n",
        "  for s, a, r in reversed(states_actions_rewards):\n",
        "    # the value of the terminal state is 0 by definition\n",
        "    # we should ignore the first state we encounter\n",
        "    # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
        "    if first:\n",
        "      first = False\n",
        "    else:\n",
        "      states_actions_returns.append((s, a, G))\n",
        "    G = r + gamma*G\n",
        "  states_actions_returns.reverse() # we want it to be in order of state visited\n",
        "  return states_actions_returns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4RXHrxBOLQW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b934259a-60b3-4c74-c24a-a688b5237276"
      },
      "source": [
        "# use the standard grid again (0 for every step) so that we can compare\n",
        "# to iterative policy evaluation\n",
        "# grid = standard_grid()\n",
        "# try the negative grid too, to see if agent will learn to go past the \"bad spot\"\n",
        "# in order to minimize number of steps\n",
        "grid = negative_grid(step_cost=-0.5)\n",
        "\n",
        "# print rewards\n",
        "print(\"rewards:\")\n",
        "print_values(grid.rewards, grid)\n",
        "print('\\n')\n",
        "\n",
        "# state -> action\n",
        "# initialize a random policy\n",
        "policy = {}\n",
        "for s in grid.actions.keys():\n",
        "  policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
        "\n",
        "print('Initial Policy:')\n",
        "print_policy(policy, grid)\n",
        "print('\\n')\n",
        "\n",
        "\n",
        "# initialize Q(s,a) and returns\n",
        "Q = {}\n",
        "returns = {} # dictionary of state -> list of returns we've received\n",
        "states = grid.all_states()\n",
        "for s in states:\n",
        "  if s in grid.actions: # not a terminal state\n",
        "    Q[s] = {}\n",
        "    for a in ALL_POSSIBLE_ACTIONS:\n",
        "      Q[s][a] = 0\n",
        "      returns[(s,a)] = []\n",
        "  else:\n",
        "    # terminal state or state we can't otherwise get to\n",
        "    pass\n",
        "\n",
        "# repeat until convergence\n",
        "deltas = []\n",
        "for t in range(5000):\n",
        "  if t % 1000 == 0:\n",
        "    print(t)\n",
        "\n",
        "  # generate an episode using pi\n",
        "  biggest_change = 0\n",
        "  states_actions_returns = play_game(grid, policy)\n",
        "\n",
        "  # calculate Q(s,a)\n",
        "  seen_state_action_pairs = set()\n",
        "  for s, a, G in states_actions_returns:\n",
        "    # check if we have already seen s\n",
        "    # called \"first-visit\" MC policy evaluation\n",
        "    sa = (s, a)\n",
        "    if sa not in seen_state_action_pairs:\n",
        "      old_q = Q[s][a]\n",
        "      returns[sa].append(G)\n",
        "      Q[s][a] = np.mean(returns[sa])\n",
        "      biggest_change = max(biggest_change, np.abs(old_q - Q[s][a]))\n",
        "      seen_state_action_pairs.add(sa)\n",
        "  deltas.append(biggest_change)\n",
        "\n",
        "  # calculate new policy pi(s) = argmax[a]{ Q(s,a) }\n",
        "  for s in policy.keys():\n",
        "    a, _ = max_dict(Q[s])\n",
        "    policy[s] = a\n",
        "\n",
        "plt.plot(deltas)\n",
        "plt.show()\n",
        "\n",
        "# find the optimal state-value function\n",
        "# V(s) = max[a]{ Q(s,a) }\n",
        "V = {}\n",
        "for s in policy.keys():\n",
        "  V[s] = max_dict(Q[s])[1]\n",
        "\n",
        "print(\"final values:\")\n",
        "print_values(V, grid)\n",
        "print(\"final policy:\")\n",
        "print_policy(policy, grid)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rewards:\n",
            "-------------------------------\n",
            "-0.50|-0.50|-0.50| 1.00|\n",
            "-------------------------------\n",
            "-0.50| 0.00|-0.50|-1.00|\n",
            "-------------------------------\n",
            "-0.50|-0.50|-0.50|-0.50|\n",
            "\n",
            "\n",
            "Initial Policy:\n",
            "Policy \n",
            "\n",
            "-----------------------------\n",
            "  U  |  D  |  U  |     |\n",
            "-----------------------------\n",
            "  L  |     |  U  |     |\n",
            "-----------------------------\n",
            "  D  |  L  |  L  |  L  |\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "final values:\n",
            "-------------------------------\n",
            "-0.26| 0.32| 1.00| 0.00|\n",
            "-------------------------------\n",
            "-0.78| 0.00| 0.23| 0.00|\n",
            "-------------------------------\n",
            "-1.24|-1.70|-1.53|-1.00|\n",
            "final policy:\n",
            "Policy \n",
            "\n",
            "-----------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "-----------------------------\n",
            "  U  |     |  U  |     |\n",
            "-----------------------------\n",
            "  U  |  L  |  U  |  U  |\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IuCCmGAOM4h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}