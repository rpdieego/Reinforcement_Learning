{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement_Learning_03.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM9WpBovQJwGvCqgxXu+0i1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rpdieego/Reinforcement_Learning/blob/master/Reinforcement_Learning_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-efU43F10EV",
        "colab_type": "text"
      },
      "source": [
        "# Monte Carlo Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dkr33qv_Okex",
        "colab_type": "text"
      },
      "source": [
        "#### Monte Carlo Policy Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXs2_sQK12KY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import relevant packages\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycWH5ukePL6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# definitions\n",
        "\n",
        "#convergence parameter\n",
        "conv_parameter = 10e-4\n",
        "\n",
        "# discount factor\n",
        "gamma = 0.9\n",
        "\n",
        "# Action Space\n",
        "ALL_POSSIBLE_ACTIONS = ('U','D','L','R')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaMlEeNQO2s7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#grid world class\n",
        "\n",
        "class Grid: # Environment\n",
        "  def __init__ (self, rows, cols, start):\n",
        "    self.rows = rows\n",
        "    self.cols = cols\n",
        "    self.i = start[0]\n",
        "    self.j = start[1]\n",
        "\n",
        "  def set(self, rewards, actions):\n",
        "    # rewards is a dict of: (i,j): r\n",
        "    # actions is a dict of: (i,j): A\n",
        "    self.rewards = rewards\n",
        "    self.actions = actions\n",
        "\n",
        "  def set_state(self,s):\n",
        "    #force state\n",
        "    self.i = s[0]\n",
        "    self.j = s[1]\n",
        "\n",
        "  def current_state(self):\n",
        "    #return the current state\n",
        "    return (self.i, self.j)\n",
        "  \n",
        "  def is_terminal(self, s):\n",
        "    #if state is not listed in the actions dictionary, it means it's a terminal state\n",
        "    return s not in self.actions\n",
        "\n",
        "  def get_next_state(self, s, a):\n",
        "    i, j = s[0], s[1]\n",
        "    if a in self.actions[(i,j)]:\n",
        "      if a == 'U':\n",
        "        i -= 1\n",
        "      elif a =='D':\n",
        "        i += 1\n",
        "      elif a == 'R':\n",
        "        j += 1\n",
        "      elif a == 'L':\n",
        "        j -= 1\n",
        "    return i,j\n",
        "\n",
        "  def move(self, action):\n",
        "    # check if legal move first\n",
        "    if action in self.actions[(self.i, self.j)]:\n",
        "      if action == 'U':\n",
        "        self.i -= 1\n",
        "      elif action == 'D':\n",
        "        self.i += 1\n",
        "      elif action == 'R':\n",
        "        self.j += 1\n",
        "      elif action == 'L':\n",
        "        self.j -= 1\n",
        "    return self.rewards.get((self.i, self.j), 0)\n",
        "\n",
        "  def undo_move(self, action):\n",
        "    if action == 'U':\n",
        "      self.i += 1\n",
        "    elif action == 'D':\n",
        "      self.i -= 1\n",
        "    elif action == 'R':\n",
        "      self.j -= 1\n",
        "    elif action == 'L':\n",
        "      self.j += 1\n",
        "    #should never happen\n",
        "    assert(self.current_state() in self.all_states())\n",
        "  \n",
        "  def game_over(self):\n",
        "    #true if in a state where no actions are possible\n",
        "    return (self.i,self.j) not in self.actions\n",
        "\n",
        "  def all_states(self):\n",
        "    #either a position that has possible next actions or a positions that yields a reward\n",
        "    return set(self.actions.keys()) | set(self.rewards.keys())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAosBpBQPAjN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define grid environment\n",
        "\n",
        "def standard_grid():\n",
        "  g = Grid(3, 4, (2,0))\n",
        "  rewards = {(0,3): 1, (1,3): -1}\n",
        "  actions = {\n",
        "      (0,0): ('D','R'),\n",
        "      (0,1): ('L','R'),\n",
        "      (0,2): ('L','D','R'),\n",
        "      (1,0): ('U','D'),\n",
        "      (1,2): ('U', 'D', 'R'),\n",
        "      (2,0): ('U','R'),\n",
        "      (2,1): ('L','R'),\n",
        "      (2,2): ('L','R','U'),\n",
        "      (2,3): ('L','U')\n",
        "  }\n",
        "  g.set(rewards, actions)\n",
        "  return g"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-T2rE2rjQ-qP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define grid environment with step penalties\n",
        "\n",
        "def negative_grid(step_cost=-0.1):\n",
        "  # in this game we want to try to minimize the number of moves\n",
        "  # so we will penalize every move\n",
        "  g = standard_grid()\n",
        "  g.rewards.update({\n",
        "    (0, 0): step_cost,\n",
        "    (0, 1): step_cost,\n",
        "    (0, 2): step_cost,\n",
        "    (1, 0): step_cost,\n",
        "    (1, 2): step_cost,\n",
        "    (2, 0): step_cost,\n",
        "    (2, 1): step_cost,\n",
        "    (2, 2): step_cost,\n",
        "    (2, 3): step_cost,\n",
        "  })\n",
        "  return g"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLPSs2j0PBC7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# auxiliar function to print values\n",
        "def print_values(V,g):\n",
        "  for i in range(g.rows):\n",
        "    print('-------------------------------')\n",
        "    for j in range(g.cols):\n",
        "      v = V.get((i,j),0)\n",
        "      if v>= 0:\n",
        "        print(\" %.2f|\" % v, end=\"\")\n",
        "      else:\n",
        "        print(\"%.2f|\" % v, end=\"\") # - sign take up an extra space\n",
        "    print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suzZ_orOQQoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# auxiliar function to print policy\n",
        "def print_policy(P,g):\n",
        "  print('Policy \\n')\n",
        "  for i in range(g.rows):\n",
        "    print('-----------------------------')\n",
        "    for j in range(g.cols):\n",
        "      a = P.get((i,j),' ')\n",
        "      print(\"  %s  |\" % a, end='')\n",
        "    print('')\n",
        "  print('\\n')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6p_tgPZQTzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_game(grid, policy):\n",
        "\n",
        "  #returns a list of states and corresponding returns\n",
        "\n",
        "  #reset game to restart at random position\n",
        "  #we need to do this, because given our current deterministic policy\n",
        "  #we would never end up at certain states, but we still want to measure their value\n",
        "\n",
        "  start_states = list(grid.actions.keys())\n",
        "  start_idx = np.random.choice(len(start_states))\n",
        "  grid.set_state(start_states[start_idx])\n",
        "\n",
        "  s = grid.current_state()\n",
        "  # list of tuples of (state, reward)\n",
        "  states_and_rewards = [(s,0)]\n",
        "  while not  grid.game_over():\n",
        "    a = policy[s]\n",
        "    r = grid.move(a)\n",
        "    s = grid.current_state()\n",
        "    states_and_rewards.append((s,r))\n",
        "\n",
        "  #Calculate teh returns by working backwards from the terminal state\n",
        "  G = 0\n",
        "  states_and_returns = []\n",
        "  first = True\n",
        "  for s,r in reversed(states_and_rewards):\n",
        "    # value of terminal state is 0 by definition\n",
        "    #we should ignores the first state we encounter (terminal)\n",
        "    # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
        "    if first:\n",
        "      first = False\n",
        "    else:\n",
        "      states_and_returns.append((s,G))\n",
        "    \n",
        "    G = r + gamma*G\n",
        "\n",
        "  # we want it to be in order of state visited\n",
        "  states_and_returns.reverse()\n",
        "\n",
        "  return states_and_returns\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUMgMF7WSW-9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "f9de7394-3fd7-4548-8855-da13b04a074c"
      },
      "source": [
        "### main\n",
        "\n",
        "# use the standard grid again (0 for every step) so that we can compare \n",
        "# to the iterative policy evaluation\n",
        "\n",
        "grid = standard_grid()\n",
        "\n",
        "# print rewards\n",
        "print('Rewards:')\n",
        "print_values(grid.rewards, grid)\n",
        "print('\\n')\n",
        "\n",
        "# state -> action\n",
        "policy = {\n",
        "    (2,0): 'U',\n",
        "    (1,0): 'U',\n",
        "    (0,0): 'R',\n",
        "    (0,1): 'R',\n",
        "    (0,2): 'R',\n",
        "    (1,2): 'R',\n",
        "    (2,1): 'R',\n",
        "    (2,2): 'R',\n",
        "    (2,3): 'U',\n",
        "}\n",
        "\n",
        "# initialize V(s) and returns\n",
        "V = {}\n",
        "#dictionary of state => list of returns we've recieved\n",
        "returns = {}\n",
        "states = grid.all_states()\n",
        "for s in states:\n",
        "  if s in grid.actions:\n",
        "    returns[s] = []\n",
        "  else:\n",
        "    #terminal state or state we can't otherwise get to\n",
        "    V[s] = 0\n",
        "\n",
        "# repeat\n",
        "for t in range(100):\n",
        "\n",
        "  #generate an episode using pi\n",
        "  states_and_returns = play_game(grid, policy)\n",
        "  seen_states = set()\n",
        "  for s, G in states_and_returns:\n",
        "    # ' first visit ' MC policy evaluation\n",
        "    # check if we already have seen s\n",
        "    if s not in seen_states:\n",
        "      returns[s].append(G)\n",
        "      V[s] = np.mean(returns[s])\n",
        "      seen_states.add(s)\n",
        "\n",
        "print('Values:')\n",
        "print_values(V,grid)\n",
        "print('\\n')\n",
        "print('Policy:')\n",
        "print_policy(policy,grid)\n",
        "print('\\n')\n",
        "\n",
        "\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rewards:\n",
            "-------------------------------\n",
            " 0.00| 0.00| 0.00| 1.00|\n",
            "-------------------------------\n",
            " 0.00| 0.00| 0.00|-1.00|\n",
            "-------------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "\n",
            "\n",
            "Values:\n",
            "-------------------------------\n",
            " 0.81| 0.90| 1.00| 0.00|\n",
            "-------------------------------\n",
            " 0.73| 0.00|-1.00| 0.00|\n",
            "-------------------------------\n",
            " 0.66|-0.81|-0.90|-1.00|\n",
            "\n",
            "\n",
            "Policy:\n",
            "Policy \n",
            "\n",
            "-----------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "-----------------------------\n",
            "  U  |     |  R  |     |\n",
            "-----------------------------\n",
            "  U  |  R  |  R  |  U  |\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRE4Xkq3f3-Q",
        "colab_type": "text"
      },
      "source": [
        "#### Monte Carlo Policy Evaluation in the Grid World\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEEAiw28f7-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_action(a):\n",
        "  #choose given a with probability 0.5\n",
        "  #choose some other a' != with probability 0.5/3\n",
        "  p = np.random.random()\n",
        "  if p < 0.5:\n",
        "    return a\n",
        "  else:\n",
        "    tmp = list(ALL_POSSIBLE_ACTIONS)\n",
        "    tmp.remove(a)\n",
        "    return np.random.choice(tmp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYmmYXlyh2KT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_game(grid, policy):\n",
        "\n",
        "  #returns a list of states and corresponding returns\n",
        "\n",
        "  #reset game to restart at random position\n",
        "  #we need to do this, because given our current deterministic policy\n",
        "  #we would never end up at certain states, but we still want to measure their value\n",
        "\n",
        "  start_states = list(grid.actions.keys())\n",
        "  start_idx = np.random.choice(len(start_states))\n",
        "  grid.set_state(start_states[start_idx])\n",
        "\n",
        "  s = grid.current_state()\n",
        "  # list of tuples of (state, reward)\n",
        "  states_and_rewards = [(s,0)]\n",
        "  while not  grid.game_over():\n",
        "    a = policy[s]\n",
        "    # possibly choose another action\n",
        "    a = random_action(a)\n",
        "    r = grid.move(a)\n",
        "    s = grid.current_state()\n",
        "    states_and_rewards.append((s,r))\n",
        "\n",
        "  #Calculate teh returns by working backwards from the terminal state\n",
        "  G = 0\n",
        "  states_and_returns = []\n",
        "  first = True\n",
        "  for s,r in reversed(states_and_rewards):\n",
        "    # value of terminal state is 0 by definition\n",
        "    #we should ignores the first state we encounter (terminal)\n",
        "    # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
        "    if first:\n",
        "      first = False\n",
        "    else:\n",
        "      states_and_returns.append((s,G))\n",
        "    \n",
        "    G = r + gamma*G\n",
        "\n",
        "  # we want it to be in order of state visited\n",
        "  states_and_returns.reverse()\n",
        "\n",
        "  return states_and_returns\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YizA2baYiTXp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "6cf5180d-b24e-49bb-ef7e-11b12af61fdd"
      },
      "source": [
        "### main\n",
        "\n",
        "# use the standard grid again (0 for every step) so that we can compare \n",
        "# to the iterative policy evaluation\n",
        "\n",
        "grid = standard_grid()\n",
        "\n",
        "# print rewards\n",
        "print('Rewards:')\n",
        "print_values(grid.rewards, grid)\n",
        "print('\\n')\n",
        "\n",
        "# state -> action\n",
        "policy = {\n",
        "    (2,0): 'U',\n",
        "    (1,0): 'U',\n",
        "    (0,0): 'R',\n",
        "    (0,1): 'R',\n",
        "    (0,2): 'R',\n",
        "    (1,2): 'U',\n",
        "    (2,1): 'L',\n",
        "    (2,2): 'U',\n",
        "    (2,3): 'L',\n",
        "}\n",
        "\n",
        "# initialize V(s) and returns\n",
        "V = {}\n",
        "#dictionary of state => list of returns we've recieved\n",
        "returns = {}\n",
        "states = grid.all_states()\n",
        "for s in states:\n",
        "  if s in grid.actions:\n",
        "    returns[s] = []\n",
        "  else:\n",
        "    #terminal state or state we can't otherwise get to\n",
        "    V[s] = 0\n",
        "\n",
        "# repeat\n",
        "for t in range(100):\n",
        "\n",
        "  #generate an episode using pi\n",
        "  states_and_returns = play_game(grid, policy)\n",
        "  seen_states = set()\n",
        "  for s, G in states_and_returns:\n",
        "    # ' first visit ' MC policy evaluation\n",
        "    # check if we already have seen s\n",
        "    if s not in seen_states:\n",
        "      returns[s].append(G)\n",
        "      V[s] = np.mean(returns[s])\n",
        "      seen_states.add(s)\n",
        "\n",
        "print('Values:')\n",
        "print_values(V,grid)\n",
        "print('\\n')\n",
        "print('Policy:')\n",
        "print_policy(policy,grid)\n",
        "print('\\n')\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rewards:\n",
            "-------------------------------\n",
            " 0.00| 0.00| 0.00| 1.00|\n",
            "-------------------------------\n",
            " 0.00| 0.00| 0.00|-1.00|\n",
            "-------------------------------\n",
            " 0.00| 0.00| 0.00| 0.00|\n",
            "\n",
            "\n",
            "Values:\n",
            "-------------------------------\n",
            " 0.49| 0.62| 0.74| 0.00|\n",
            "-------------------------------\n",
            " 0.38| 0.00| 0.21| 0.00|\n",
            "-------------------------------\n",
            " 0.31| 0.25| 0.14| 0.02|\n",
            "\n",
            "\n",
            "Policy:\n",
            "Policy \n",
            "\n",
            "-----------------------------\n",
            "  R  |  R  |  R  |     |\n",
            "-----------------------------\n",
            "  U  |     |  U  |     |\n",
            "-----------------------------\n",
            "  U  |  L  |  U  |  L  |\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDDB3LfIiqRt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}